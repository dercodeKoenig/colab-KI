{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan-TPU-128",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xGU6GWAq0KAcES45Jr5BNogaQrs2kVNs",
      "authorship_tag": "ABX9TyMXv8rVgVj6ake3JKKjE3/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dercodeKoenig/colab-KI/blob/main/gan_TPU_128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9ZGIMvw-Aan"
      },
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import cv2\n",
        "from IPython import display\n",
        "\n",
        "train_images_cartoon = []\n",
        "small_images_cartoon = []\n",
        "\n",
        "\n",
        "\n",
        "if os.path.isfile(\"npfilet1.npy\") and os.path.isfile(\"npfilet2.npy\") and os.path.isfile(\"npfilet3.npy\") and os.path.isfile(\"npfilet4.npy\") and os.path.isfile(\"npfiles.npy\"):\n",
        "    pass\n",
        "else:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/dercodeKoenig/cartoon-faces.git\n",
        "  train_images_cartoon = []\n",
        "  small_images_cartoon = []\n",
        "  z=0\n",
        "  total = len(os.listdir(\"cartoon-faces\"))\n",
        "  max=512*100\n",
        "  files = os.listdir(\"cartoon-faces\")\n",
        "  while(True):\n",
        "    if(z>=max):\n",
        "      break\n",
        "    print(\"\\r\" + str(z) + \" / \" + str(51200),end=\"\")\n",
        "    try:\n",
        "      img = cv2.imread(\"cartoon-faces/\"+files[z])\n",
        "\n",
        "      img1 = img.copy()\n",
        "      img3 = cv2.resize(img1, (32,32))\n",
        "      img3 = cv2.blur(img3,(7,7))\n",
        "      small_images_cartoon.append(img3)\n",
        "\n",
        "      img1 = img.copy()\n",
        "      img2 = cv2.resize(img1, (128,128))\n",
        "      train_images_cartoon.append(img2)\n",
        "    \n",
        "    except:\n",
        "      print(\"error: \")\n",
        "      try:\n",
        "        print(\" cartoon-faces/\"+files[z])\n",
        "      except:\n",
        "        pass\n",
        "      max+=1\n",
        "\n",
        "    z+=1\n",
        "  print(\"\")\n",
        "  %cd /content/drive/MyDrive/\n",
        "  train_images_cartoon1 = train_images_cartoon[int(len(train_images_cartoon)/4*0):int(len(train_images_cartoon)/4*1)]\n",
        "  train_images_cartoon2 = train_images_cartoon[int(len(train_images_cartoon)/4*1):int(len(train_images_cartoon)/4*2)]\n",
        "  train_images_cartoon3 = train_images_cartoon[int(len(train_images_cartoon)/4*2):int(len(train_images_cartoon)/4*3)]\n",
        "  train_images_cartoon4 = train_images_cartoon[int(len(train_images_cartoon)/4*3):int(len(train_images_cartoon)/4*4)]\n",
        "  \n",
        "  print(\"1\")\n",
        "  train_images_cartoon1 = np.array(train_images_cartoon1,dtype=object)\n",
        "  train_images_cartoon1 = train_images_cartoon1.reshape(train_images_cartoon1.shape[0], 128, 128, 3).astype('float32')\n",
        "  train_images_cartoon1 = (train_images_cartoon1 - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "  np.save(\"npfilet1\", train_images_cartoon1)\n",
        "  del train_images_cartoon1\n",
        "  \n",
        "  print(\"2\")\n",
        "  train_images_cartoon2 = np.array(train_images_cartoon2,dtype=object)\n",
        "  train_images_cartoon2 = train_images_cartoon2.reshape(train_images_cartoon2.shape[0], 128, 128, 3).astype('float32')\n",
        "  train_images_cartoon2 = (train_images_cartoon2 - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "  np.save(\"npfilet2\", train_images_cartoon2)\n",
        "  del train_images_cartoon2\n",
        "\n",
        "  print(\"3\")\n",
        "  train_images_cartoon3 = np.array(train_images_cartoon3,dtype=object)\n",
        "  train_images_cartoon3 = train_images_cartoon3.reshape(train_images_cartoon3.shape[0], 128, 128, 3).astype('float32')\n",
        "  train_images_cartoon3 = (train_images_cartoon3 - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "  np.save(\"npfilet3\", train_images_cartoon3)\n",
        "  del train_images_cartoon3\n",
        "  \n",
        "  print(\"4\") \n",
        "  train_images_cartoon4 = np.array(train_images_cartoon4,dtype=object)\n",
        "  train_images_cartoon4 = train_images_cartoon4.reshape(train_images_cartoon4.shape[0], 128, 128, 3).astype('float32')\n",
        "  train_images_cartoon4 = (train_images_cartoon4 - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "  np.save(\"npfilet4\", train_images_cartoon4)\n",
        "  del train_images_cartoon4\n",
        "  \n",
        "  small_images_cartoon = np.array(small_images_cartoon,dtype=object)\n",
        "  small_images_cartoon = small_images_cartoon.reshape(small_images_cartoon.shape[0], 32, 32, 3).astype('float32')\n",
        "  small_images_cartoon = (small_images_cartoon - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "  np.save(\"npfiles\", small_images_cartoon)\n",
        "\n",
        "  print(\"ok\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOWhbMKI5jB2",
        "outputId": "43f0bf0f-1396-4a6d-fede-de7c1c9c9e91"
      },
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import cv2\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "small_images_cartoon = np.load(\"npfiles.npy\")\n",
        "test_img = small_images_cartoon[0:20]\n",
        "print(small_images_cartoon.shape)\n",
        "\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "BATCH_SIZE_PER_REPLICA = 4\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * 8 #8TPUs\n",
        "\n",
        "try:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "except:\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "small_images_cartoon = tf.data.Dataset.from_tensor_slices((small_images_cartoon)).batch(GLOBAL_BATCH_SIZE)  \n",
        "small_images_cartoon = strategy.experimental_distribute_dataset(small_images_cartoon)\n",
        "\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(32,32,3)))\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(64, 5))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(32, 5))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(8*8*16, use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "    \n",
        "    model.add(layers.Reshape((8,8,16)))\n",
        "    print(model.output_shape)\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5),padding=\"same\", strides=(2, 2), use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5),padding=\"same\", strides=(2, 2), use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5),padding=\"same\", strides=(2, 2), use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "    \n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5),padding=\"same\", strides=(2, 2), use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "  \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "    \n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, 5,padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(3,5,padding=\"same\", activation='tanh', use_bias=False))\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Input(shape=(128,128,3)))\n",
        "    \n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(32, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(32, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(1, 1)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "\n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "  discriminator = make_discriminator_model()\n",
        "  generator = make_generator_model()\n",
        "\n",
        "  discriminator.summary()\n",
        "  generator.summary()\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False,reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "  def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    #print(\"disc_loss:\",total_loss)\n",
        "    return  tf.nn.compute_average_loss(total_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "\n",
        "  def _gaussian_kernel(kernel_size, sigma, n_channels, dtype):\n",
        "    x = tf.range(-kernel_size // 2 + 1, kernel_size // 2 + 1, dtype=dtype)\n",
        "    g = tf.math.exp(-(tf.pow(x, 2) / (2 * tf.pow(tf.cast(sigma, dtype), 2))))\n",
        "    g_norm2d = tf.pow(tf.reduce_sum(g), 2)\n",
        "    g_kernel = tf.tensordot(g, g, axes=0) / g_norm2d\n",
        "    g_kernel = tf.expand_dims(g_kernel, axis=-1)\n",
        "    return tf.expand_dims(tf.tile(g_kernel, (1, 1, n_channels)), axis=-1)\n",
        "\n",
        "\n",
        "  def apply_blur(img):\n",
        "    blur = _gaussian_kernel(19, 30, 3, img.dtype)\n",
        "    img = tf.nn.depthwise_conv2d(img, blur, [1,1,1,1], 'SAME')\n",
        "    return img\n",
        "\n",
        "\n",
        "  def generator_loss(fake_output,norm,genr):\n",
        "    genr=tf.image.resize(genr,[32,32])\n",
        "    norm=apply_blur(norm)\n",
        "    genr=apply_blur(genr)\n",
        "    total_loss=tf.nn.compute_average_loss(((norm-genr)*(norm-genr)))+tf.nn.compute_average_loss((cross_entropy(tf.ones_like(fake_output), fake_output)))\n",
        "   # print(\"gen_loss:\",total_loss)\n",
        "    return  (total_loss)\n",
        "\n",
        "  generator_optimizer = tf.keras.optimizers.Adam(0.000005)\n",
        "  discriminator_optimizer = tf.keras.optimizers.Adam(0.00002)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step_cartoon(cartoons,dataset_seed):\n",
        "      #print(\"train_step\")\n",
        "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_cartoon_tape :\n",
        "        #print(\"generator...\")\n",
        "        generated_images = generator(dataset_seed, training=True)\n",
        "        #print(\"disc_real...\")\n",
        "        real_output_cartoon = discriminator(cartoons, training=True)\n",
        "        #print(\"disc_fake...\")\n",
        "        fake_output_cartoon = discriminator(generated_images, training=True)\n",
        "        #print(\"gen_loss...\")\n",
        "        gen_loss = generator_loss(fake_output_cartoon,dataset_seed,generated_images)\n",
        "        #print(\"disc_loss...\")\n",
        "        disc_cartoon_loss = discriminator_loss(real_output_cartoon, fake_output_cartoon)\n",
        "      #print(\"gen_grad...\")\n",
        "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "      #print(\"disc_grad...\")\n",
        "      gradients_of_discriminator = disc_cartoon_tape.gradient(disc_cartoon_loss, discriminator.trainable_variables)\n",
        "      \n",
        "      #print(\"opt...\")\n",
        "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "      \n",
        "        \n",
        "  \n",
        "\n",
        "def train(dataset_seed,dataset_cartoon, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    ii=0\n",
        "    for image_batch in dataset_cartoon:\n",
        "      for seed_batch in dataset_seed:\n",
        "        ii+=1\n",
        "        print(\"\\r\" + str(ii),end=\"\")\n",
        "        strategy.run(train_step_cartoon,args=(image_batch,seed_batch,))\n",
        "\n",
        "        if(ii%1000==0):\n",
        "          print(\"start test\")\n",
        "          generate_images(generator,test_img)\n",
        "        if(ii%10000==0):\n",
        "          print(\"save chkp\")\n",
        "          generator.save_weights('gen_weights.h5', overwrite=True)\n",
        "          discriminator_cartoon.save_weights('disc_weights.h5', overwrite=True)\n",
        "          generate_images(generator,test_img)\n",
        "  \n",
        "      \n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "\n",
        "starttime = time.process_time()\n",
        "\n",
        "\n",
        "def generate_images(model, test_input):    \n",
        "  predictions = model(test_input,training=False)\n",
        "  predictions = np.array(predictions)\n",
        "    \n",
        "#  test_input = apply_blur(test_input)\n",
        "  test_input = np.array(test_input,dtype= 'float32')\n",
        "  test_input = (test_input+1)*127.5\n",
        "\n",
        "#  predictions=apply_blur(predictions)\n",
        "\n",
        "  generated_images = np.array(predictions,dtype= 'float32')\n",
        "#  print(generated_images)\n",
        "  generated_images = (generated_images+1)*127.5\n",
        "  cz=0\n",
        "  cz2 =0\n",
        "  heighest = 0\n",
        "  while(True):\n",
        "      heighest +=1\n",
        "      if(os.path.exists(\"outputs/0/\"+str(heighest)+\".jpg\")==False):\n",
        "        break\n",
        "\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  for i in generated_images:\n",
        "    plt.subplot(4, 10, cz2+1)\n",
        "    plt.imshow(np.array(cv2.cvtColor(test_input[cz], cv2.COLOR_BGR2RGB),dtype= 'int32'))\n",
        "    plt.subplot(4, 10, cz2+2)\n",
        "    plt.imshow(np.array(cv2.cvtColor(i, cv2.COLOR_BGR2RGB),dtype= 'int32'))\n",
        "    plt.axis('off')\n",
        "    cz+=1\n",
        "    cz2+=2\n",
        "  \n",
        "  display.clear_output(wait=True)\n",
        "  plt.show()\n",
        "  global starttime\n",
        "  end = time.process_time()\n",
        "  print(end - starttime)\n",
        "  starttime = time.process_time()\n",
        "\n",
        "\n",
        "try:\n",
        "  generator.load_weights('gen_weights.h5')\n",
        "  discriminator.load_weights('disc_weights.h5')\n",
        "except:\n",
        "  print(\"no weights\")\n",
        "\n",
        "files=[\"npfilet2.npy\",\"npfilet3.npy\",\"npfilet4.npy\",\"npfilet1.npy\"]\n",
        "for file in files:\n",
        "  train_images_cartoon = np.load(file)\n",
        "  train_images_cartoon1=train_images_cartoon[0:int(train_images_cartoon.shape[0]/2)]\n",
        "  train_images_cartoon2=train_images_cartoon[int(train_images_cartoon.shape[0]/2):int(train_images_cartoon1.shape[0])]\n",
        "\n",
        "  train_images_cartoon1 = tf.data.Dataset.from_tensor_slices((train_images_cartoon1)).batch(GLOBAL_BATCH_SIZE)  \n",
        "  train_images_cartoon1 = strategy.experimental_distribute_dataset(train_images_cartoon1)\n",
        "\n",
        "  train_images_cartoon2 = tf.data.Dataset.from_tensor_slices((train_images_cartoon2)).batch(GLOBAL_BATCH_SIZE)  \n",
        "  train_images_cartoon2 = strategy.experimental_distribute_dataset(train_images_cartoon2)\n",
        "\n",
        "  train(small_images_cartoon,train_images_cartoon1, 1)\n",
        "  print(\"save chkp\")\n",
        "  generator.save_weights('gen_weights.h5', overwrite=True)\n",
        "  discriminator.save_weights('disc_weights.h5', overwrite=True)\n",
        "  train(small_images_cartoon,train_images_cartoon2, 1)\n",
        "  print(\"save chkp\")\n",
        "  generator.save_weights('gen_weights.h5', overwrite=True)\n",
        "  discriminator.save_weights('disc_weights.h5', overwrite=True)\n",
        "  del train_images_cartoon1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n",
            "(51200, 32, 32, 3)\n",
            "All devices:  []\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.45.169.178:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.45.169.178:8470\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAhTCnp6RBkH",
        "outputId": "c7eca0a6-9550-465c-a197-177cd869855c"
      },
      "source": [
        "print(\"save chkp\")\n",
        "generator.save_weights('gen_weights.h5', overwrite=True)\n",
        "discriminator.save_weights('disc_weights.h5', overwrite=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save chkp\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}