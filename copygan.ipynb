{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "copygan",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1iX_UmeVHBlGvC_JE3yRfwnON0Nm6mjxx",
      "authorship_tag": "ABX9TyNuJcI5JKkhEP8d9UoIGcxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dercodeKoenig/colab-KI/blob/main/copygan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cScWnx5EvQHg",
        "outputId": "f78b00e4-3cf5-4409-fe90-3da730876100"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztj41_INgeZb",
        "outputId": "5133eac3-d26a-4ad7-aa63-3ab1c90c3496"
      },
      "source": [
        "%cd /content\n",
        "!cp drive/MyDrive/gen_weights.h5 gen_weights.h5\n",
        "!cp drive/MyDrive/disc_weights.h5 disc_weights.h5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2P5sGFJgkhL"
      },
      "source": [
        "%%writefile kaggle.json\n",
        "{\"username\":\"lutze3\",\"key\":\"c1b954a36f5402b976973271cb154a2f\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDR2CDIhgk3s"
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d rikotheboss/nppreprocessed\n",
        "!unzip nppreprocessed.zip\n",
        "!mkdir input\n",
        "!mv npf* input/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T49u4Kg1gmC9"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import cv2\n",
        "from IPython import display\n",
        "from threading import Thread\n",
        " \n",
        "t0= time.time()\n",
        " \n",
        "resume = True\n",
        "gen_weights = \"gen_weights.h5\"\n",
        "disc_weights = \"disc_weights.h5\"\n",
        "datadir=\"input/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34l6n-U-gnje"
      },
      "source": [
        "GLOBAL_BATCH_SIZE=16\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    GLOBAL_BATCH_SIZE=GLOBAL_BATCH_SIZE*8\n",
        "except:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "#display.clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWVllahwgo4q"
      },
      "source": [
        "namest = os.listdir(datadir)\n",
        "print(namest)\n",
        "small_images_cartoon = np.load(datadir+namest[0])\n",
        "test_imgs = small_images_cartoon[0:8]\n",
        "test_imgs = np.array(test_imgs)\n",
        "del small_images_cartoon\n",
        "        \n",
        "train_images_cartoon = 0\n",
        "zlr=0\n",
        " \n",
        "def load_next():\n",
        "    global train_images_cartoon, small_images_cartoon, zlr\n",
        "    while(\"npf\" not in namest[zlr]):\n",
        "        zlr+=1\n",
        "        if(zlr==len(namest)):\n",
        "            zlr=0\n",
        "    \n",
        " \n",
        "    \n",
        "    train_images_cartoon = np.load(datadir+namest[zlr])\n",
        "    #print(\"new data: \",train_images_cartoon.shape)\n",
        "    train_images_cartoon = tf.keras.layers.experimental.preprocessing.RandomZoom((0,0.1),(0,0.1),fill_mode='constant',fill_value=1)(train_images_cartoon)\n",
        "    train_images_cartoon = tf.keras.layers.experimental.preprocessing.RandomTranslation((-0.2,0.2), (-0.2,0.2),fill_mode='constant',fill_value=1)(train_images_cartoon)\n",
        "        \n",
        "    train_images_cartoon = tf.data.Dataset.from_tensor_slices((train_images_cartoon)).batch(GLOBAL_BATCH_SIZE) \n",
        "    train_images_cartoon = strategy.experimental_distribute_dataset(train_images_cartoon)\n",
        "\n",
        "    zlr+=1\n",
        "    if(zlr==len(namest)):\n",
        "      zlr=0\n",
        " \n",
        "print(\"load first files\")\n",
        "load_next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WZHp-m0gqbr"
      },
      "source": [
        "def make_generator_model():\n",
        "    inputa=tf.keras.layers.Input(shape=(128,128,3))\n",
        "    \n",
        "    x = layers.Conv2D(64, 5)(inputa)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2D(128, 3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        " \n",
        "    x = layers.Conv2D(256, 3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, 3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, 3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, 3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    \n",
        "    x = layers.Conv2D(512, 3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(512, 3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        " \n",
        "    x = layers.Dense(100)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Dense(8*8*128)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Reshape((8,8,128))(x)\n",
        " \n",
        " \n",
        "    x = layers.Conv2D(500, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(500, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(500, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(500, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2DTranspose(400, (5, 5), strides=(2, 2),padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2D(400, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2D(400, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(400, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(400, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        " \n",
        "    x = layers.Conv2DTranspose(300, (5, 5), strides=(2, 2),padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    \n",
        "    x = layers.Conv2D(300, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2D(300, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(300, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(300, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    \n",
        "    x = layers.Conv2DTranspose(200, (5, 5),padding=\"same\", strides=(2, 2))(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2D(200, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(200, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(200, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(200, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2DTranspose(100, (5, 5),padding=\"same\", strides=(2, 2))(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2D(100, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        " \n",
        "    x = layers.Conv2D(100, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(100, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(100, 3,padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    outputs = layers.Conv2D(3,3,padding=\"same\",strides=(1, 1), activation='tanh')(x)\n",
        " \n",
        "    model = tf.keras.Model(inputs=inputa, outputs=outputs, name=\"generator_model\")\n",
        "    return model\n",
        " \n",
        "def make_discriminator_model():\n",
        "    inputa = layers.Input(shape=(128,128,3))\n",
        " \n",
        "    x = layers.Conv2D(50,7)(inputa)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        " \n",
        "    x = layers.Conv2D(100,5)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Conv2D(100,5)(x)\n",
        "    x = layers.Conv2D(100,5)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    x = layers.Conv2D(200,3,strides=(2,2))(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Conv2D(200,3)(x)\n",
        "    x = layers.Conv2D(200,3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    x = layers.Conv2D(300,3,strides=(2,2))(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Conv2D(300,3)(x)\n",
        "    x = layers.Conv2D(300,3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    x = layers.Conv2D(400,3,strides=(2,2))(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Conv2D(400,3)(x)\n",
        "    x = layers.Conv2D(400,3)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    x = layers.Dense(128)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    \n",
        "    x = layers.Dense(64)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        " \n",
        "    output = layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "    model = tf.keras.Model(inputs=inputa, outputs=output, name=\"discriminator_model\")\n",
        "    return model\n",
        " \n",
        " \n",
        "with strategy.scope():\n",
        "    generator = make_generator_model()\n",
        "    #generator.summary()\n",
        " \n",
        "    discriminator = make_discriminator_model()\n",
        "    #discriminator.summary()\n",
        " \n",
        "if(resume):\n",
        "    generator.load_weights(gen_weights)\n",
        "    discriminator.load_weights(disc_weights)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLn_PSEBpMJr"
      },
      "source": [
        "generator_optimizer =   tf.keras.optimizers.Adam(0.0000005)\n",
        "disc_optimizer =        tf.keras.optimizers.Adam(0.0000005)\n",
        " \n",
        "#generator_optimizer =  tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)\n",
        "#disc_optimizer =       tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1tZoLv7grjQ"
      },
      "source": [
        "mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
        " \n",
        " \n",
        "def _gaussian_kernel(kernel_size, sigma, n_channels, dtype):\n",
        "        x = tf.range(-kernel_size // 2 + 1, kernel_size // 2 + 1, dtype=dtype)\n",
        "        g = tf.math.exp(-(tf.pow(x, 2) / (2 * tf.pow(tf.cast(sigma, dtype), 2))))\n",
        "        g_norm2d = tf.pow(tf.reduce_sum(g), 2)\n",
        "        g_kernel = tf.tensordot(g, g, axes=0) / g_norm2d\n",
        "        g_kernel = tf.expand_dims(g_kernel, axis=-1)\n",
        "        return tf.expand_dims(tf.tile(g_kernel, (1, 1, n_channels)), axis=-1)\n",
        " \n",
        " \n",
        "def apply_blur(img):\n",
        "        blur = _gaussian_kernel(3, 2, 3, img.dtype)\n",
        "        img = tf.nn.depthwise_conv2d(img, blur, [1,1,1,1], 'SAME')\n",
        "        return img\n",
        " \n",
        "def calc_discriminator_loss(true_output,fake_output):\n",
        "    total_loss = mse(tf.ones_like(true_output),true_output)+mse(tf.zeros_like(fake_output),fake_output)\n",
        " \n",
        "  \n",
        "    return total_loss\n",
        " \n",
        " \n",
        "def calc_generator_loss(genr,norm, disc_output):\n",
        "        normb=apply_blur(norm)\n",
        "        genrb=apply_blur(genr)\n",
        "        #pploss=tf.math.reduce_sum((normb-genrb)*(normb-genrb)/(128*128*3))\n",
        "        pploss=tf.math.reduce_sum((normb-genrb)*(normb-genrb)/(128*128*3)+(norm-genr)*(norm-genr)/(128*128*3))\n",
        " \n",
        "        dloss=mse(tf.ones_like(disc_output),disc_output)\n",
        "        total_loss=dloss+pploss*2\n",
        " \n",
        "        return total_loss\n",
        " \n",
        " \n",
        " \n",
        "@tf.function\n",
        "def train_step_cartoon(cartoons):\n",
        "        with tf.GradientTape() as tape, tf.GradientTape() as dtape:\n",
        "            generated_images = generator(cartoons, training=True)\n",
        " \n",
        "            noise1 = np.random.normal(0, .05, generated_images.shape)\n",
        "            noise2 = np.random.normal(0, .05, cartoons.shape)\n",
        " \n",
        "            generated_images_noise = generated_images + noise1\n",
        "            cartoons_noise = cartoons + noise2\n",
        "            \n",
        "            d_true = discriminator(cartoons_noise,training=True)\n",
        "            d_false = discriminator(generated_images_noise,training=True)\n",
        " \n",
        "            \n",
        "            discriminator_loss = calc_discriminator_loss(d_true,d_false)\n",
        "            gen_loss = calc_generator_loss(generated_images,cartoons,d_false)\n",
        "            \n",
        "      \n",
        "        gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        " \n",
        "        gradients_of_disc = dtape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
        "        disc_optimizer.apply_gradients(zip(gradients_of_disc, discriminator.trainable_variables))\n",
        "      \n",
        "        \n",
        "def train(dataset_cartoon):\n",
        "    #x=Thread(target=load_next)\n",
        "    #x.daemon=True\n",
        "    #x.start()\n",
        "    load_next()\n",
        "    for i in range(5):\n",
        "        for seed_batch in dataset_cartoon:\n",
        "            strategy.run(train_step_cartoon,args=(seed_batch,))\n",
        " \n",
        "test_imgs = np.array(test_imgs)\n",
        " \n",
        "    \n",
        "fig = plt.figure(figsize=(100,10))\n",
        "for i in range(test_imgs.shape[0]):\n",
        "        img = cv2.cvtColor(test_imgs[i],cv2.COLOR_BGR2RGB)\n",
        "        plt.subplot(1, 10, i+1)\n",
        "        plt.imshow((img+1)/2)\n",
        "plt.show()\n",
        " \n",
        "def generate_images():    \n",
        "    predictions = generator(test_imgs)\n",
        "    generated_images = np.array(predictions,dtype= 'float32')\n",
        "    fig = plt.figure(figsize=(100,10))\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        img = cv2.cvtColor(generated_images[i],cv2.COLOR_BGR2RGB)\n",
        "        plt.subplot(1, 10, i+1)\n",
        "        plt.imshow((img+1)/2)\n",
        "    plt.show()\n",
        " \n",
        "    \n",
        "generate_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwt5tFsXgswX"
      },
      "source": [
        "tzsl = 0\n",
        "while(True):\n",
        "    t=train_images_cartoon\n",
        "   \n",
        "    \n",
        "    train(t)\n",
        "    tzsl+=1\n",
        "    if(tzsl%3==0):\n",
        "      generate_images() \n",
        "      generator.save_weights('/content/drive/MyDrive/gen_weights.h5', overwrite=True)\n",
        "      discriminator.save_weights('/content/drive/MyDrive/disc_weights.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRygxvF0cZw0"
      },
      "source": [
        "generator.save_weights('/content/drive/MyDrive/gen_weights.h5', overwrite=True)\n",
        "discriminator.save_weights('/content/drive/MyDrive/disc_weights.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yv2bUOngwGy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}